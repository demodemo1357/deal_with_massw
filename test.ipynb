{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt.txt', 'r') as file:\n",
    "    prompt = file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"\"\"\n",
    "Title: Attention Is All You Need\n",
    "Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
    "\"\"\"\n",
    "\n",
    "example_output = \"\"\"{\n",
    "  \"Context\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing sequence transduction models connect the encoder and decoder through an attention mechanism.\",\n",
    "  \"Key Idea\": \"The authors propose a simple network architecture called Transformer based solely on attention mechanisms and dispenses with recurrence and convolutions.\",\n",
    "  \"Method\": \"The authors perform experiments on the WMT 2014 English-to-German and English-to-French translation task. The authors apply the proposed model to English constituency parsing both with large and limited training data.\",\n",
    "  \"Outcome\": \"The proposed model achieves a BLEU score of 28.4 on the WMT 2014 English-to-French translation task. The proposed model achieves a BLEU score of 41.8 on the WMT 2014 English-to-German translation task after training for 3.5 days on 8 GPUs.\",\n",
    "  \"Future Impact\": \"N/A\"\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title = corpus_clean_data[0]['title']\n",
    "abstract = corpus_clean_data[0]['abstract']\n",
    "full_text = f\"Title: {title}\\nAbstract: {abstract}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS\n",
      "Abstract: We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the perframe baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency. â€¡ Equal contribution.\n"
     ]
    }
   ],
   "source": [
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "\n",
    "    AIMessage,\n",
    "\n",
    "    HumanMessage,\n",
    "\n",
    "    SystemMessage\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content= prompt),\n",
    "    HumanMessage(content= example_prompt),\n",
    "    AIMessage(content = example_output),\n",
    "    HumanMessage(content=full_text)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\demo123\\AppData\\Local\\Temp\\ipykernel_21296\\1559093686.py:3: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm(messages)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 1433\n",
      "\tPrompt Tokens: 1246\n",
      "\tCompletion Tokens: 187\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00029909999999999995\n"
     ]
    }
   ],
   "source": [
    "print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Context\": \"Generating videos from text is particularly challenging due to computational costs, limited high-quality text-video data, and the variable length of videos. Previous video generation methods have struggled with these issues.\",\n",
      "  \"Key Idea\": \"The authors introduce Phenaki, a model that learns video representation using a tokenizer that compresses video into discrete tokens and employs causal attention to handle variable-length videos.\",\n",
      "  \"Method\": \"The model uses a bidirectional masked transformer conditioned on pre-computed text tokens to generate video tokens from text, which are then de-tokenized to create the actual video. Joint training on a large corpus of image-text pairs and a smaller number of video-text examples is utilized.\",\n",
      "  \"Outcome\": \"Phenaki demonstrates the ability to generate arbitrarily long videos conditioned on time-variable prompts, achieving better spatio-temporal consistency compared to previous per-frame baselines.\",\n",
      "  \"Future Impact\": \"N/A\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00029909999999999995\n"
     ]
    }
   ],
   "source": [
    "print(cb.total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=[]\n",
    "for i, paper in enumerate(corpus_clean_data):\n",
    "    title = corpus_clean_data[i]['title']\n",
    "    abstract = corpus_clean_data[i]['abstract']\n",
    "    full_text = f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    messages = [\n",
    "                SystemMessage(content= prompt),\n",
    "                HumanMessage(content= example_prompt),\n",
    "                AIMessage(content = example_output),\n",
    "                HumanMessage(content=full_text)\n",
    "            ]  \n",
    "    batch.append(messages)\n",
    "    if i==1:\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm.batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Context\": \"Generating videos from text is particularly challenging due to computational costs, limited quantities of high-quality text-video data, and the variable length of videos. Previous video generation methods have struggled with these issues.\",\n",
      "  \"Key Idea\": \"The authors introduce Phenaki, a model that can generate realistic videos from sequences of textual prompts, leveraging a new tokenizer that uses causal attention in time to handle variable-length videos.\",\n",
      "  \"Method\": \"The model uses a bidirectional masked transformer conditioned on pre-computed text tokens to generate video tokens, which are then de-tokenized to create the actual video. Joint training on a large corpus of image-text pairs along with a smaller number of video-text examples is employed to enhance generalization.\",\n",
      "  \"Outcome\": \"Phenaki demonstrates the ability to generate arbitrarily long videos conditioned on time variable prompts, achieving better spatio-temporal consistency compared to previous per-frame baselines.\",\n",
      "  \"Future Impact\": \"N/A\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(result[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Context\": \"Generative Adversarial Networks (GANs) achieve state-of-the-art results on various generative tasks but are known to be unstable and prone to missing modes in high-dimensional spaces.\",\n",
      "  \"Key Idea\": \"The authors propose several regularization techniques to stabilize GAN training and ensure a more equitable distribution of probability mass across the modes of the data generating distribution.\",\n",
      "  \"Method\": \"The authors introduce several ways of regularizing the GAN objective and demonstrate these techniques' effectiveness in stabilizing GAN training and addressing the missing modes problem.\",\n",
      "  \"Outcome\": \"The proposed regularizers dramatically stabilize the training of GAN models and help distribute probability mass fairly across the modes during early training phases.\",\n",
      "  \"Future Impact\": \"N/A\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(result[1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005642999999999999\n"
     ]
    }
   ],
   "source": [
    "print(cb.total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "results=[]\n",
    "costs=[]\n",
    "for i, paper in enumerate(corpus_clean_data['full_paper']):\n",
    "    paper = corpus_clean_data[i]['full_paper']\n",
    "    messages = [\n",
    "    SystemMessage(content= prompt),\n",
    "    HumanMessage(content=paper)\n",
    "    ]\n",
    "    with get_openai_callback() as cb:\n",
    "        result = llm(messages)\n",
    "        cost = cb.total_cost\n",
    "    id = corpus_clean_data[i]['corpusid']\n",
    "    text = result.content\n",
    "    results.append({\n",
    "        \"corpusid\": id,\n",
    "        \"response\": text\n",
    "    })\n",
    "    costs.append(cost)\n",
    "    if i==1:\n",
    "        break\n",
    "\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
