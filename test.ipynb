{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt.txt', 'r') as file:\n",
    "    prompt = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = corpus_clean_data[0]['full_paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "\n",
    "    AIMessage,\n",
    "\n",
    "    HumanMessage,\n",
    "\n",
    "    SystemMessage\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content= prompt),\n",
    "    HumanMessage(content=full_text)\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\demo123\\AppData\\Local\\Temp\\ipykernel_9264\\1559093686.py:3: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm(messages)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 16748\n",
      "\tPrompt Tokens: 16537\n",
      "\tCompletion Tokens: 211\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00260715\n"
     ]
    }
   ],
   "source": [
    "print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Context\": \"Generating high-quality videos from text is challenging due to limited data and high computational costs. Previous models either handle fixed-length outputs or treat video frames independently, leading to poor temporal coherence.\",\n",
      "    \"Key Idea\": \"Phenaki introduces a novel model capable of generating variable-length, temporally coherent videos conditioned on sequential textual prompts, making it the first work to explore story-based conditional video generation.\",\n",
      "    \"Method\": \"The study employs the C-ViViT encoder-decoder architecture for compressing videos into discrete tokens and a bidirectional masked transformer to generate video tokens from text, allowing for efficient processing of variable-length videos.\",\n",
      "    \"Outcome\": \"Phenaki demonstrates the capability to generate coherent and diverse videos based on dynamic text inputs, achieving competitive performance on various video generation tasks, including zero-shot evaluations against existing models.\",\n",
      "    \"Future Impact\": \"The authors foresee Phenaki as a creative tool for storytelling and content creation, with potential improvements in video generation quality as larger and more diverse datasets are developed.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00260715\n"
     ]
    }
   ],
   "source": [
    "print(cb.total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00262095\n"
     ]
    }
   ],
   "source": [
    "testcost = cb.total_cost\n",
    "print(testcost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "costs=[]\n",
    "for i, paper in enumerate(corpus_clean_data['full_paper']):\n",
    "    paper = corpus_clean_data[i]['full_paper']\n",
    "    messages = [\n",
    "    SystemMessage(content= prompt),\n",
    "    HumanMessage(content=paper)\n",
    "    ]\n",
    "    with get_openai_callback() as cb:\n",
    "        result = llm(messages)\n",
    "        cost = cb.total_cost\n",
    "    id = corpus_clean_data[i]['corpusid']\n",
    "    text = result.content\n",
    "    results.append({\n",
    "        \"corpusid\": id,\n",
    "        \"response\": text\n",
    "    })\n",
    "    costs.append(cost)\n",
    "    if i==1:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'corpusid': 252715594, 'response': '```json\\n{\\n    \"Context\": \"Generating high-quality videos from text remains a challenging task due to the scarcity of high-quality text-video pairs and the significant computational costs involved. Prior methods have primarily focused on fixed-length videos or treated video frames independently, which leads to poor temporal coherence.\",\\n    \"Key Idea\": \"This study introduces Phenaki, a novel model that generates variable-length videos conditioned on open-domain textual descriptions, utilizing a new encoder-decoder architecture called C-ViViT, which compresses videos into discrete tokens while maintaining temporal coherence.\",\\n    \"Method\": \"Phenaki employs a bidirectional masked transformer for generating video tokens from text, combined with joint training on a large dataset of image-text pairs and a smaller set of video-text examples, facilitating improved generalization.\",\\n    \"Outcome\": \"The Phenaki model demonstrates the ability to generate long, temporally coherent videos based on sequential prompts, achieving better spatio-temporal consistency than existing methods, and performs competitively on video prediction tasks despite not being designed specifically for them.\",\\n    \"Future Impact\": \"The authors anticipate that Phenaki will enhance creative applications in various fields such as art, design, and content creation, and they highlight the potential for future research to address bias and ethical concerns associated with generative models.\"\\n}\\n```'}, {'corpusid': 13002849, 'response': '```json\\n{\\n    \"Context\": \"Generative Adversarial Networks (GANs) have demonstrated strong performance in generative tasks but face significant challenges such as instability during training and the missing modes problem, where the generator fails to capture the full diversity of the data distribution, often collapsing to a few dominant modes.\",\\n    \"Key Idea\": \"This paper introduces mode regularization techniques to stabilize GAN training and address the missing modes problem by incorporating additional regularizers based on geometric metrics, which provide more stable gradients and encourage fair distribution of probability mass across the data modes.\",\\n    \"Method\": \"The authors propose a two-step training procedure called manifold-diffusion training, where the first step aligns the generation manifold with the real data manifold using a geometric metric loss, and the second step ensures fair probability mass distribution across modes using a mode regularizer.\",\\n    \"Outcome\": \"Experiments show that the regularized GAN models significantly improve the diversity of generated samples and reduce the number of missing modes compared to baseline GAN models, demonstrating both enhanced stability and quality in the generated outputs.\",\\n    \"Future Impact\": \"The proposed techniques for stabilizing GAN training and addressing the missing modes issue could lead to further advancements in generative modeling, encouraging research into new regularization strategies and improved architectures for GANs.\"\\n}\\n```'}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0026365499999999997, 0.0018834]\n"
     ]
    }
   ],
   "source": [
    "print(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results.json\", \"w\") as json_file:\n",
    "    json.dump(results, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
